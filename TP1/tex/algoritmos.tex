\section{Detalles implementativos}
Utilizamos el lenguaje \textit{Python}, y seguimos el paradigma orientado a objetos. Creamos una clase \textit{Perceptrón Múltiple} que representa la red neuronal, y en otro archivo definimos la interfaz con el usuario. La clase se inicializa con los parámetros esperables de \textit{learning rate}, capas, etc., y mediante funciones públicas permite entrenar con datasets, con diversos métodos, predecir un output. 

Para ambos problemas a resolver se usaron las mismas funciones, siendo diferentes la carga de los datos. Como función de activación usamos la sigmoidea bipolar. 

La interfaz y los detalles de cada ejercicio se explicará en secciones posteriores. 

Las funciones que se ofrecen son las siguientes:
\begin{itemize}
\item \textbf{activation} Calcula el output de al red dado un $X_{h}$
\item \textbf{correction} Calcula los $\Delta$ W dado un target $Z_{h}$
\item \textbf{adaptation} Aplica los $\Delta$ W 
\item \textbf{batch} 
\item \textbf{incremental}
\item \textbf{holdout} Dado un porcentaje, reserva un set para validación. 
\end{itemize}

Se pensó en implementar la técnica de \textit{early stopping} pero se evitó para no aumentar la complejidad del código.

\subsection{Algoritmos}
En esta seccion describimos los pseudo-codigos de los algoritmos que utilizamos para resolver ambos ejercicios.
%Pseudocodigo de Activation
\begin{center}
\noindent\fbox{
\begin{minipage}{0.5\textwidth}
\begin{algorithm}[H]
 activation($X_h$)\;
 $Y_1$ = $X_h$\;
 \For{j de 2 a L}{
    $Y_j = f_j(Y_{j-1} * w_j)$\;
  }
  return $Y_l$\;
 \caption{Activation}
\end{algorithm}
\end{minipage}
}
\end{center}

%Pseudocodigo de Correction
\begin{center}
\noindent\fbox{
\begin{minipage}{0.5\textwidth}
\begin{algorithm}[H]
 correction($Z_k$)\;
  E = (Z-$Y_l$)\;
  e = $||E||^2$\;
  \For{j de L a 2}{
    $D = E * F_j^{'} (Y_{j-1}*W_j)$\;
    $dw_j = dw_j + \eta (Y_{j-1}^{T}*D)$\;
    $E = D*w_j^{t}$\;
  }
  return e\;
 \caption{Correction}
\end{algorithm}
\end{minipage}
}
\end{center}

%Pseudocodigo de Adaptation
\begin{center}
\noindent\fbox{
\begin{minipage}{0.5\textwidth}
\begin{algorithm}[H]
 adaptation()\;
  \For{j de 2 a L}{
    $W_j = W_j + dw_j$\;
    $dw_j = w_j + dw_j$\;
  }
 \caption{Adaptation}
\end{algorithm}
\end{minipage}
}
\end{center}

%Pseudocodigo de Trainning Batch
\begin{center}
\noindent\fbox{
\begin{minipage}{0.5\textwidth}
\begin{algorithm}[H]
 batch(X,Z)\;
  $e = 0$\;
  \For{h de 1 a P}{
    activation($X_h$)\;
    e = e + correction($Z_h$)\;
  }
  adaptation()\;
  return e\;
 \caption{Batch}
\end{algorithm}
\end{minipage}
}
\end{center}

%Pseudocodigo de Trainning Incremental
\begin{center}
\noindent\fbox{
\begin{minipage}{0.5\textwidth}
\begin{algorithm}[H]
 incremental(X,Z)\;
  $e = 0$\;
  \For{h de 1 a P}{
    activation($X_h$)\;
    e = e + correction($Z_h$)\;
    adaptation()\;
  }
  
  return e\;
 \caption{Incremental}
\end{algorithm}
\end{minipage}
}
\end{center}

%Pseudocodigo de Holdout
\begin{center}
\noindent\fbox{
\begin{minipage}{0.5\textwidth}
\begin{algorithm}[H]
 holdout($\epsilon$,T)\;
  $e_t = 1$\;
  $t = 0$\;
  $e_v = 1$\;
  v = ?? \;
  \While{$e_t > \epsilon $ y t $<$ T}{
    $e_t$ = trainning($Y_{[:v]}, Z_{[:v]}$)\;
    $e_v$ = testing($X_{[:v]}, Z_{[:v]}$)\;
    t = t+1
  }
  return $e_v,t$\;
 \caption{Holdout}
\end{algorithm}
\end{minipage}
}

\end{center}